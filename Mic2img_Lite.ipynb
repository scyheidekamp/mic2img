{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Mic2img Lite\n",
        "Works on T4 so no Google Colab subscription needed.\n",
        "You do need an OpenAI and StabilityAI API key."
      ],
      "metadata": {
        "id": "TWOvUZNg97_x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QncwEeHbBNji"
      },
      "outputs": [],
      "source": [
        "#@title Check GPU status\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cC982X14mr4u"
      },
      "outputs": [],
      "source": [
        "#@title Mount Google Drive\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "drive_mounted = False\n",
        "gdrive_fpath = '.'\n",
        "local_path = '/content/'\n",
        "\n",
        "\n",
        "\n",
        "#@markdown Mounting your google drive is optional.\n",
        "#@markdown If you mount your drive, the text and image files will be stored on it.\n",
        "\n",
        "mount_gdrive = True # @param{type:\"boolean\"}\n",
        "if mount_gdrive and not drive_mounted:\n",
        "    from google.colab import drive\n",
        "\n",
        "    gdrive_mountpoint = '/content/drive/' #@param{type:\"string\"}\n",
        "    gdrive_subdirectory = 'MyDrive/mic2img' #@param{type:\"string\"}\n",
        "    gdrive_fpath = str(Path(gdrive_mountpoint) / gdrive_subdirectory)\n",
        "    try:\n",
        "        drive.mount(gdrive_mountpoint, force_remount = True)\n",
        "        !mkdir -p {gdrive_fpath}\n",
        "        %cd {gdrive_fpath}\n",
        "        local_path = gdrive_fpath\n",
        "        drive_mounted = True\n",
        "    except OSError:\n",
        "        print(\n",
        "            \"If you received an OSError and your drive\"\n",
        "            \" was already mounted, ignore it.\"\n",
        "            )\n",
        "        raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcK9fDCU-ltk"
      },
      "outputs": [],
      "source": [
        "#@title Install dependencies\n",
        "!pip install openai\n",
        "!pip install stability-sdk\n",
        "!pip install keybert\n",
        "!pip install transformers -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQ_fEdS-d5uw"
      },
      "outputs": [],
      "source": [
        "#@title Install keyword & emotion models\n",
        "from keybert import KeyBERT\n",
        "from transformers import RobertaTokenizerFast, TFRobertaForSequenceClassification, pipeline\n",
        "kw_model = KeyBERT()\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(\"arpanghoshal/EmoRoBERTa\")\n",
        "model = TFRobertaForSequenceClassification.from_pretrained(\"arpanghoshal/EmoRoBERTa\")\n",
        "emotion = pipeline('text-classification', model='arpanghoshal/EmoRoBERTa', return_all_scores=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Global settings\n",
        "\n",
        "#open_ai_organisation = \"Personal\" #@param {type:\"string\"}\n",
        "open_ai_key = \"123456\" #@param {type:\"string\"}\n",
        "stability_ai_key = \"123456\" #@param {type:\"string\"}\n",
        "\n",
        "prompt_modifier = \"drawn illustration\"#@param {type:\"string\"}\n",
        "directory_name = \"test_0\"#@param {type:\"string\"}\n",
        "\n",
        "amount_of_emotions= 4#@param {type:\"slider\", min:0, max:10, step:1}"
      ],
      "metadata": {
        "id": "D4Ajo0MNir4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Server related stuff\n",
        "hostName = \"0.0.0.0\"\n",
        "serverPort = 8000\n",
        "\n",
        "\n",
        "# Python 3 server example\n",
        "from http.server import BaseHTTPRequestHandler, HTTPServer\n",
        "from google.colab.output import eval_js\n",
        "import time\n",
        "import base64\n",
        "\n",
        "class MyServer(BaseHTTPRequestHandler):\n",
        "  def do_GET(self):\n",
        "    fbase64_string = None\n",
        "    if os.path.exists(f\"{gdrive_fpath}/{directory_name}/latest.png\"):\n",
        "      f = open(f\"{gdrive_fpath}/{directory_name}/latest.png\", \"rb\")\n",
        "      fbase64_string = base64.b64encode(f.read()).decode('ascii')\n",
        "    self.send_response(200)\n",
        "    self.send_header(\"Content-type\", \"text/html\")\n",
        "    self.end_headers()\n",
        "    self.wfile.write(bytes(\"<html style=\\\"background-color:#000000;text-align:center;\\\"><head><title>Latest image</title></head>\", \"utf-8\"))\n",
        "    self.wfile.write(bytes(\"<body>\", \"utf-8\"))\n",
        "    if fbase64_string is not None:\n",
        "      self.wfile.write(bytes(\"<img style=\\\"width:auto;height:100%;max-height:vh;\\\" src=\\\"data:image/jpeg;base64,\" + fbase64_string + \"\\\"/>\", \"utf-8\"))\n",
        "    self.wfile.write(bytes(\"<script> setTimeout(() => { location.reload() }, 2000) </script>\", \"utf-8\"))\n",
        "    self.wfile.write(bytes(\"</body></html>\", \"utf-8\"))\n",
        "\n",
        "  def log_message(self, format, *args):\n",
        "    return\n",
        "\n",
        "print(eval_js(\"google.colab.kernel.proxyPort(\" + str(serverPort) + \")\"))"
      ],
      "metadata": {
        "id": "xwHNh1nPd89L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Click this link above to display the images in a new window\n",
        "\n",
        "Refresh the page after starting the loop below, it should start displaying images within a minute"
      ],
      "metadata": {
        "id": "4stOrEy2faGY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP26Io4HC2Fn"
      },
      "source": [
        "# Start the loop\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFuw4vnO_mRc"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import ipywidgets as widgets\n",
        "from threading import Thread\n",
        "from queue import Queue\n",
        "import openai\n",
        "from IPython.display import Javascript, display\n",
        "from google.colab import output\n",
        "from base64 import b64decode\n",
        "import time\n",
        "import subprocess\n",
        "import os\n",
        "import signal\n",
        "import sys\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import multiprocessing\n",
        "import stability_sdk.interfaces.gooseai.generation.generation_pb2 as generation\n",
        "from stability_sdk.client import StabilityInference\n",
        "\n",
        "\n",
        "RECORD = \"\"\"\n",
        "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
        "const b2text = blob => new Promise(resolve => {\n",
        "  const reader = new FileReader()\n",
        "  reader.onloadend = e => resolve(e.srcElement.result)\n",
        "  reader.readAsDataURL(blob)\n",
        "})\n",
        "var record = time => new Promise(async resolve => {\n",
        "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
        "  recorder = new MediaRecorder(stream)\n",
        "  chunks = []\n",
        "  recorder.ondataavailable = e => chunks.push(e.data)\n",
        "  recorder.start()\n",
        "  await sleep(time)\n",
        "  recorder.onstop = async ()=>{\n",
        "    blob = new Blob(chunks)\n",
        "    text = await b2text(blob)\n",
        "    resolve(text)\n",
        "  }\n",
        "  recorder.stop()\n",
        "})\n",
        "\"\"\"\n",
        "\n",
        "#openai.organization = open_ai_organisation\n",
        "openai.api_key = open_ai_key\n",
        "stability_key =  stability_ai_key\n",
        "counter = int(0)\n",
        "\n",
        "def record(sec=3):\n",
        "  display(Javascript(RECORD))\n",
        "  s = output.eval_js('record(%d)' % (sec*1000))\n",
        "  b = b64decode(s.split(',')[1])\n",
        "  return b\n",
        "\n",
        "messages = Queue()\n",
        "recordings = Queue()\n",
        "\n",
        "textblock = widgets.Output()\n",
        "\n",
        "def prompt_to_image(i, prompt):\n",
        "    api = StabilityInference(key=stability_key, verbose=True)\n",
        "    if i > 0:\n",
        "        prev = Image.open(f\"{directory_name}/{(counter-1):04d}.png\")\n",
        "    else:\n",
        "        prev = None\n",
        "    answers = api.generate(prompt=prompt, init_image=prev)\n",
        "    for resp in answers:\n",
        "        for artifact in resp.artifacts:\n",
        "            if artifact.type == generation.ARTIFACT_IMAGE:\n",
        "                image = Image.open(BytesIO(artifact.binary))\n",
        "                return image\n",
        "    return None\n",
        "\n",
        "def speech_recognition(textblock):\n",
        "  global counter\n",
        "  while not messages.empty():\n",
        "    frames = recordings.get()\n",
        "\n",
        "    if len(frames) > 0:\n",
        "\n",
        "      if os.path.exists(\"/content/audio.webm\"):\n",
        "        os.remove(\"/content/audio.webm\")\n",
        "      if os.path.exists(\"/content/audio.wav\"):\n",
        "        os.remove(\"/content/audio.wav\")\n",
        "\n",
        "      with open('/content/audio.webm','wb') as f:\n",
        "        f.write(frames)\n",
        "        command = ['ffmpeg', '-i', \"/content/audio.webm\", \"/content/audio.wav\"]\n",
        "        subprocess.run(command,stdout=subprocess.PIPE,stdin=subprocess.PIPE)\n",
        "      with open('/content/audio.wav', 'rb') as f:\n",
        "        t = openai.Audio.translate(\"whisper-1\", f)\n",
        "        try:\n",
        "          work_text = t[\"text\"]\n",
        "        except:\n",
        "          return None\n",
        "        print(work_text)\n",
        "\n",
        "        sentence = Prompt(work_text)\n",
        "        keywords = sentence.extract_keywords()\n",
        "        emo = sentence.extract_emotions()\n",
        "        final_prompt = keywords + emo + prompt_modifier\n",
        "        print(final_prompt)\n",
        "        image = prompt_to_image(counter, final_prompt)\n",
        "        if not image:\n",
        "            return None\n",
        "\n",
        "        if not os.path.exists(directory_name):\n",
        "          os.mkdir(directory_name)\n",
        "\n",
        "        image.save(f\"{directory_name}/latest.png\")\n",
        "        image = image.save(f\"{directory_name}/{counter:04d}.png\")\n",
        "\n",
        "        with open(f\"{directory_name}/recorded.txt\", 'a') as f:\n",
        "          f.write(\"\\n\" + work_text)\n",
        "\n",
        "        display(image)\n",
        "\n",
        "        counter += 1\n",
        "        frames = []\n",
        "\n",
        "\n",
        "class Prompt:\n",
        "\n",
        "  def __init__(self, sentence):\n",
        "    self.sentence = sentence\n",
        "\n",
        "  def extract_keywords(self):\n",
        "    keyword_list = kw_model.extract_keywords(self.sentence, keyphrase_ngram_range=(1, 1))\n",
        "    result = [x[0] for x in keyword_list]\n",
        "    return str(result)\n",
        "\n",
        "  def extract_emotions(self):\n",
        "    emotion_labels, = emotion(self.sentence)\n",
        "    sorted_emotions = sorted(emotion_labels, key=lambda item: item['score'], reverse=True)\n",
        "    emo_result = []\n",
        "    for i in sorted_emotions[:amount_of_emotions]:\n",
        "      val = list(i.values())\n",
        "      text = (val[0])\n",
        "      emo_result.append(text)\n",
        "    return str(emo_result)\n",
        "\n",
        "def start_recording():\n",
        "  print('start recording')\n",
        "  messages.put(True)\n",
        "\n",
        "  transcribe = Thread(target=speech_recognition, args=(textblock,))\n",
        "  transcribe.start()\n",
        "\n",
        "  while True:\n",
        "    frames = record(30)\n",
        "    print('finished recording')\n",
        "    recordings.put(frames)\n",
        "    time.sleep(0.1)\n",
        "\n",
        "def start_server():\n",
        "  webServer = HTTPServer((hostName, serverPort), MyServer)\n",
        "  print(\"Server started http://%s:%s\" % (hostName, serverPort))\n",
        "\n",
        "  try:\n",
        "      webServer.serve_forever()\n",
        "  except:\n",
        "      pass\n",
        "\n",
        "  webServer.server_close()\n",
        "  print(\"Server stopped.\")\n",
        "\n",
        "server = multiprocessing.Process(target=start_server)\n",
        "server.start()\n",
        "\n",
        "def signal_handler(sig, frame):\n",
        "  server.terminate()  # sends a SIGTERM\n",
        "  sys.exit(0)\n",
        "\n",
        "\n",
        "\n",
        "signal.signal(signal.SIGINT, signal_handler)\n",
        "\n",
        "start_recording()\n",
        "display(textblock)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuClass": "premium",
      "gpuType": "T4"
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}