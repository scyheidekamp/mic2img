{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QncwEeHbBNji"
      },
      "outputs": [],
      "source": [
        "#@title Check GPU status\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cC982X14mr4u"
      },
      "outputs": [],
      "source": [
        "#@title Mount Google Drive\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "drive_mounted = False\n",
        "gdrive_fpath = '.'\n",
        "local_path = '/content/'\n",
        "\n",
        "\n",
        "\n",
        "#@markdown Mounting your google drive is optional.\n",
        "#@markdown If you mount your drive, the text and image files will be stored on it.\n",
        "\n",
        "mount_gdrive = True # @param{type:\"boolean\"}\n",
        "if mount_gdrive and not drive_mounted:\n",
        "    from google.colab import drive\n",
        "\n",
        "    gdrive_mountpoint = '/content/drive/' #@param{type:\"string\"}\n",
        "    gdrive_subdirectory = 'MyDrive/mic2emo2img' #@param{type:\"string\"}\n",
        "    gdrive_fpath = str(Path(gdrive_mountpoint) / gdrive_subdirectory)\n",
        "    try:\n",
        "        drive.mount(gdrive_mountpoint, force_remount = True)\n",
        "        !mkdir -p {gdrive_fpath}\n",
        "        %cd {gdrive_fpath}\n",
        "        local_path = gdrive_fpath\n",
        "        drive_mounted = True\n",
        "    except OSError:\n",
        "        print(\n",
        "            \"If you received an OSError and your drive\"\n",
        "            \" was already mounted, ignore it.\"\n",
        "            )\n",
        "        raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcK9fDCU-ltk"
      },
      "outputs": [],
      "source": [
        "#@title Install dependencies\n",
        "!pip install ffmpeg-python\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install diffusers==0.11.1\n",
        "!pip install transformers scipy ftfy accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhmJBJIJ6bfd"
      },
      "outputs": [],
      "source": [
        "#@title Install FFmpeg v5.0 \n",
        "import os, uuid, re, IPython\n",
        "import ipywidgets as widgets\n",
        "import time\n",
        "\n",
        "from glob import glob\n",
        "from google.colab import output, drive\n",
        "\n",
        "from IPython.display import clear_output\n",
        "import os, sys, urllib.request\n",
        "HOME = os.path.expanduser(\"~\")\n",
        "pathDoneCMD = f'{HOME}/doneCMD.sh'\n",
        "if not os.path.exists(f\"{HOME}/.ipython/ttmg.py\"):\n",
        "    hCode = \"https://raw.githubusercontent.com/yunooooo/gcct/master/res/ttmg.py\"\n",
        "    urllib.request.urlretrieve(hCode, f\"{HOME}/.ipython/ttmg.py\")\n",
        "\n",
        "!git clone https://github.com/XniceCraft/ffmpeg-colab.git\n",
        "!chmod 755 ./ffmpeg-colab/install\n",
        "!./ffmpeg-colab/install\n",
        "clear_output()\n",
        "print('Installation finished!')\n",
        "!rm -fr /content/ffmpeg-colab\n",
        "!ffmpeg -version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56Jpera-UU2M"
      },
      "outputs": [],
      "source": [
        "#@title Install Whisper speech recognition\n",
        "!pip install git+https://github.com/openai/whisper.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etEVfb3wSj75"
      },
      "source": [
        "#Install Stable Diffusion\n",
        "\n",
        "Choose a stable diffusion model by taking a huggingface model-card link and removing everything except for the name/model\n",
        "\n",
        "i.e. huggingface.co/stabilityai/stable-diffusion-2-1 becomes stabilityai/stable-diffusion-2-1.\n",
        "\n",
        "This is what you need to add to the \"model_card\"\n",
        "\n",
        "Check out https://huggingface.co/sd-dreambooth-library for custom Stable Diffusion models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIw3-P1ISjZI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from diffusers import StableDiffusionPipeline\n",
        "\n",
        "model_card = \"stabilityai/stable-diffusion-2-1\"#@param{type:\"string\"}\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_card, torch_dtype=torch.float16)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITTJ0fsrTMXM"
      },
      "outputs": [],
      "source": [
        "pipe = pipe.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wEGT9aldhG_"
      },
      "outputs": [],
      "source": [
        "#@title Install keywords and emotions\n",
        "!pip install keybert\n",
        "!pip install transformers -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQ_fEdS-d5uw"
      },
      "outputs": [],
      "source": [
        "from keybert import KeyBERT\n",
        "from transformers import RobertaTokenizerFast, TFRobertaForSequenceClassification, pipeline\n",
        "kw_model = KeyBERT()\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(\"arpanghoshal/EmoRoBERTa\")\n",
        "model = TFRobertaForSequenceClassification.from_pretrained(\"arpanghoshal/EmoRoBERTa\")\n",
        "emotion = pipeline('text-classification', model='arpanghoshal/EmoRoBERTa', return_all_scores=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Global settings\n",
        "\n",
        "#@markdown Width and height should be divisable by 8\n",
        "width = 768 #@param {type:\"integer\"}\n",
        "height = 512 #@param {type:\"integer\"}\n",
        "\n",
        "prompt_modifier = \"beautifully drawn watercolor illustration\"#@param {type:\"string\"}\n",
        "negative_prompt= \"people, person, human, text\"#@param {type:\"string\"}\n",
        "\n",
        "directory_name = \"czechtest\"#@param {type:\"string\"}\n",
        "\n",
        "amount_of_emotions= 4#@param {type:\"slider\", min:0, max:10, step:1}"
      ],
      "metadata": {
        "id": "D4Ajo0MNir4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Server related stuff\n",
        "hostName = \"0.0.0.0\"\n",
        "serverPort = 8000\n",
        "\n",
        "\n",
        "# Python 3 server example\n",
        "from http.server import BaseHTTPRequestHandler, HTTPServer\n",
        "from google.colab.output import eval_js\n",
        "import time\n",
        "import base64\n",
        "\n",
        "class MyServer(BaseHTTPRequestHandler):\n",
        "  def do_GET(self):\n",
        "    fbase64_string = None\n",
        "    if os.path.exists(f\"{gdrive_fpath}/{directory_name}/latest.png\"):\n",
        "      f = open(f\"{gdrive_fpath}/{directory_name}/latest.png\", \"rb\")\n",
        "      fbase64_string = base64.b64encode(f.read()).decode('ascii')\n",
        "    self.send_response(200)\n",
        "    self.send_header(\"Content-type\", \"text/html\")\n",
        "    self.end_headers()\n",
        "    self.wfile.write(bytes(\"<html style=\\\"background-color:#000000;text-align:center;\\\"><head><title>Latest image</title></head>\", \"utf-8\"))\n",
        "    self.wfile.write(bytes(\"<body>\", \"utf-8\"))\n",
        "    if fbase64_string is not None:\n",
        "      self.wfile.write(bytes(\"<img style=\\\"width:auto;height:100%;max-height:vh;\\\" src=\\\"data:image/jpeg;base64,\" + fbase64_string + \"\\\"/>\", \"utf-8\"))\n",
        "    self.wfile.write(bytes(\"<script> setTimeout(() => { location.reload() }, 2000) </script>\", \"utf-8\"))\n",
        "    self.wfile.write(bytes(\"</body></html>\", \"utf-8\"))\n",
        "\n",
        "  def log_message(self, format, *args):\n",
        "    return\n",
        "\n",
        "print(eval_js(\"google.colab.kernel.proxyPort(\" + str(serverPort) + \")\"))"
      ],
      "metadata": {
        "id": "xwHNh1nPd89L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Click this link above to display the images in a new window\n",
        "\n",
        "Refresh the page after starting the loop below, it should start displaying images within a minute"
      ],
      "metadata": {
        "id": "4stOrEy2faGY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP26Io4HC2Fn"
      },
      "source": [
        "# Start the loop\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFuw4vnO_mRc"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import ipywidgets as widgets\n",
        "from threading import Thread\n",
        "from queue import Queue\n",
        "\n",
        "from IPython.display import Javascript, display\n",
        "from google.colab import output\n",
        "from base64 import b64decode\n",
        "import time\n",
        "import subprocess\n",
        "import whisper\n",
        "import os\n",
        "import signal\n",
        "import sys\n",
        "from PIL import Image\n",
        "import os\n",
        "import multiprocessing\n",
        "\n",
        "RECORD = \"\"\"\n",
        "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
        "const b2text = blob => new Promise(resolve => {\n",
        "  const reader = new FileReader()\n",
        "  reader.onloadend = e => resolve(e.srcElement.result)\n",
        "  reader.readAsDataURL(blob)\n",
        "})\n",
        "var record = time => new Promise(async resolve => {\n",
        "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
        "  recorder = new MediaRecorder(stream)\n",
        "  chunks = []\n",
        "  recorder.ondataavailable = e => chunks.push(e.data)\n",
        "  recorder.start()\n",
        "  await sleep(time)\n",
        "  recorder.onstop = async ()=>{\n",
        "    blob = new Blob(chunks)\n",
        "    text = await b2text(blob)\n",
        "    resolve(text)\n",
        "  }\n",
        "  recorder.stop()\n",
        "})\n",
        "\"\"\"\n",
        "\n",
        "def record(sec=3):\n",
        "  display(Javascript(RECORD))\n",
        "  s = output.eval_js('record(%d)' % (sec*1000))\n",
        "  b = b64decode(s.split(',')[1])\n",
        "  return b\n",
        "\n",
        "messages = Queue()\n",
        "recordings = Queue()\n",
        "\n",
        "textblock = widgets.Output()\n",
        "\n",
        "\n",
        "def speech_recognition(textblock):\n",
        "  counter = int(0)\n",
        "  while not messages.empty():\n",
        "    frames = recordings.get()\n",
        "    \n",
        "    if len(frames) > 0:\n",
        "\n",
        "      if os.path.exists(\"/content/audio.webm\"):\n",
        "        os.remove(\"/content/audio.webm\")\n",
        "      if os.path.exists(\"/content/audio.wav\"):\n",
        "        os.remove(\"/content/audio.wav\")\n",
        "\n",
        "      with open('/content/audio.webm','wb') as f:\n",
        "        f.write(frames)\n",
        "        command = ['ffmpeg', '-i', \"/content/audio.webm\", \"/content/audio.wav\"]\n",
        "        subprocess.run(command,stdout=subprocess.PIPE,stdin=subprocess.PIPE)\n",
        "    \n",
        "        model = whisper.load_model(\"large\")\n",
        "        \n",
        "\n",
        "        audio = whisper.load_audio(\"/content/audio.wav\")\n",
        "        audio = whisper.pad_or_trim(audio)\n",
        "\n",
        "        mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "        _, probs = model.detect_language(mel)\n",
        "        language = max(probs, key=probs.get)\n",
        "        print(f\"Detected language: {max(probs, key=probs.get)}\")\n",
        "        options = whisper.DecodingOptions(fp16=False)\n",
        "        result = whisper.decode(model, mel, options)\n",
        "        if language != \"en\":\n",
        "          translate = model.transcribe(audio, task = 'translate')\n",
        "          print(\"translated: \")\n",
        "          work_text = translate[\"text\"]\n",
        "          print(work_text)\n",
        "        else:\n",
        "          work_text = result.text\n",
        "          print(work_text)\n",
        "        \n",
        "        sentence = Prompt(work_text)\n",
        "        keywords = sentence.extract_keywords()\n",
        "        emo = sentence.extract_emotions()\n",
        "        final_prompt = keywords + emo + prompt_modifier\n",
        "        print(final_prompt)\n",
        "        image = pipe(final_prompt, negative_prompt=negative_prompt, height=height, width=width).images[0] \n",
        "\n",
        "        if not os.path.exists(directory_name):\n",
        "          os.mkdir(directory_name)\n",
        "        image.save(f\"{directory_name}/latest.png\")\n",
        "        image = image.save(f\"{directory_name}/{counter:04d}.png\")\n",
        "\n",
        "        with open(f\"{directory_name}/recorded.txt\", 'a') as f:\n",
        "          f.write(\"\\n\" + work_text)\n",
        "\n",
        "        display(image)\n",
        "\n",
        "        counter += 1\n",
        "        frames = []\n",
        "\n",
        "class Prompt:\n",
        "\n",
        "  def __init__(self, sentence):\n",
        "    self.sentence = sentence\n",
        "\n",
        "  def extract_keywords(self):\n",
        "    keyword_list = kw_model.extract_keywords(self.sentence, keyphrase_ngram_range=(1, 1))\n",
        "    result = [x[0] for x in keyword_list]\n",
        "    return str(result)\n",
        "\n",
        "  def extract_emotions(self):\n",
        "    emotion_labels, = emotion(self.sentence)\n",
        "    sorted_emotions = sorted(emotion_labels, key=lambda item: item['score'], reverse=True)\n",
        "    emo_result = []\n",
        "    for i in sorted_emotions[:amount_of_emotions]:\n",
        "      val = list(i.values())\n",
        "      text = (val[0])\n",
        "      emo_result.append(text)\n",
        "    return str(emo_result)\n",
        "\n",
        "def start_recording():\n",
        "  print('start recording')\n",
        "  messages.put(True)\n",
        "\n",
        "  transcribe = Thread(target=speech_recognition, args=(textblock,))\n",
        "  transcribe.start()\n",
        "\n",
        "  while True:\n",
        "    frames = record(30) \n",
        "    print('finished recording')\n",
        "    recordings.put(frames)\n",
        "    time.sleep(0.1)\n",
        "\n",
        "def start_server():\n",
        "  webServer = HTTPServer((hostName, serverPort), MyServer)\n",
        "  print(\"Server started http://%s:%s\" % (hostName, serverPort))\n",
        "\n",
        "  try:\n",
        "      webServer.serve_forever()\n",
        "  except:\n",
        "      pass\n",
        "\n",
        "  webServer.server_close()\n",
        "  print(\"Server stopped.\")\n",
        "\n",
        "server = multiprocessing.Process(target=start_server)\n",
        "server.start()\n",
        "\n",
        "def signal_handler(sig, frame):\n",
        "  server.terminate()  # sends a SIGTERM\n",
        "  sys.exit(0)\n",
        "\n",
        "\n",
        "signal.signal(signal.SIGINT, signal_handler)\n",
        "\n",
        "start_recording()\n",
        "display(textblock)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}